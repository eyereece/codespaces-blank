{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "We will write two helper functions to initialize the parameters for your model:\n",
    "* The first function will be used to initialize parameters for a two layer model.\n",
    "* The second one generalizes this initialization process to L layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters of the 2-layer neural network\n",
    "\"\"\"\n",
    "args:\n",
    "    n_x         : size of the input layer\n",
    "    n_h         : size of the hidden layer\n",
    "    n_y         : size of the output layer\n",
    "returns:\n",
    "    parameters  : python dict containing parameters (W1, b1, W2, b2)\n",
    "\"\"\"\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3, 2, 1)\n",
    "\n",
    "print(\"W1 = \" + str (parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str (parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str (parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str (parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization for a deeper L-layer neural netowkr is more complicated because there are more weight matrices and bias vectors.\n",
    "\n",
    "Here's an implementation for L = 1 (one layer neural network). It should inspire you to implement the general case.\n",
    "\n",
    "if L == 1:\n",
    "\n",
    "    parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "    \n",
    "    parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        layer_dims      : python array (list) containing the dimensions of each layer\n",
    "    returns:\n",
    "        parameters \n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[1], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]\n",
      " [-0.01244123 -0.00626417 -0.00803766 -0.02419083]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5, 4, 3])\n",
    "\n",
    "print(\"W1 = \" + str (parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str (parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str (parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str (parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
