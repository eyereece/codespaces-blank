{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from testCases import *\n",
    "from dnn_utils import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "from public_tests import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "We will write two helper functions to initialize the parameters for your model:\n",
    "* The first function will be used to initialize parameters for a two layer model.\n",
    "* The second one generalizes this initialization process to L layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parameters of the 2-layer neural network\n",
    "\"\"\"\n",
    "args:\n",
    "    n_x         : size of the input layer\n",
    "    n_h         : size of the hidden layer\n",
    "    n_y         : size of the output layer\n",
    "returns:\n",
    "    parameters  : python dict containing parameters (W1, b1, W2, b2)\n",
    "\"\"\"\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    np.random.seed(1)\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3, 2, 1)\n",
    "\n",
    "print(\"W1 = \" + str (parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str (parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str (parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str (parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization for a deeper L-layer neural netowkr is more complicated because there are more weight matrices and bias vectors.\n",
    "\n",
    "Here's an implementation for L = 1 (one layer neural network). It should inspire you to implement the general case.\n",
    "\n",
    "if L == 1:\n",
    "\n",
    "    parameters[\"W\" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01\n",
    "    \n",
    "    parameters[\"b\" + str(L)] = np.zeros((layer_dims[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        layer_dims      : python array (list) containing the dimensions of each layer\n",
    "    returns:\n",
    "        parameters \n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[1], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]\n",
      " [-0.01244123 -0.00626417 -0.00803766 -0.02419083]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5, 4, 3])\n",
    "\n",
    "print(\"W1 = \" + str (parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str (parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str (parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str (parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation Module\n",
    "\n",
    "Now that we have initialized our parameters, we can do the forward propagation module. We'll start by implementing some basic functions that we can use again later when implementing the model. Now, we'll complete three functions in this order:\n",
    "* LINEAR\n",
    "* LINEAR -> ACTIVATION where ACTIVATION will be either ReLU or Sigmoid\n",
    "* [LINEAR -> RELU] X (L-1) -> LINEAR -> SIGMOID (whole model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        A       : activations from previous layer (size of prev layer, m examples)\n",
    "        W       : weights matrix (size of current layer, size of prev layer)\n",
    "        b       : bias vector (size of current layer, 1)\n",
    "    returns:\n",
    "        Z       : input of activation function (pre-activation parameter)\n",
    "        cache   : a tuple containing args; stored for back-prop\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "t_A, t_W, t_b = linear_forward_test_case()\n",
    "t_Z, t_linear_cache = linear_forward(t_A, t_W, t_b)\n",
    "print(\"Z = \" + str(t_Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Activation Forward\n",
    "\n",
    "In this notebook, we'll use two activation function:\n",
    "* sigmoid   # A, activation_cache = sigmoid(Z)\n",
    "* ReLU      # A, activation_cache = relu(Z)\n",
    "\n",
    "For added convenience, we're going to group two function (linear and Activation) into one function (LINEAR -> ACTIVATION). Hence, we'll implement a function that does the LINEAR forward step, followed by an ACTIVATION forward step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "t_A_prev, t_W, t_b = linear_activation_forward_test_case()\n",
    "\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation=\"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(t_A))\n",
    "\n",
    "t_A, t_linear_activation_cache = linear_activation_forward(t_A_prev, t_W, t_b, activation=\"relu\")\n",
    "print(\"With ReLU: A = \" + str(t_A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L-Layer Model\n",
    "\n",
    "For even more convenience when implementing the L-layer Neural Net, we will need a function that replicates the previous one (linear_activation_forward with RELU) L-1 times, then follows that with one linear_activation_forwardwith SIGMOID.\n",
    "\n",
    "[LINEAR -> RELU] X (L-1) -> LINEAR -> SIGMOID* model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2        # number of layers in the neural network\n",
    "\n",
    "    # implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    # the for loop starts at 1 because layer 0 is the input\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    # implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n"
     ]
    }
   ],
   "source": [
    "t_X, t_parameters = L_model_forward_test_case_2hidden()\n",
    "t_AL, t_caches = L_model_forward(t_X, t_parameters)\n",
    "\n",
    "print(\"AL = \" + str(t_AL))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "we can now implement forward and backward propagation. We need to compute the cost, in order to check whether our model is actually learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    cost = -(np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL)))/m\n",
    "    cost = np.squeeze(cost) # to make sure our cost's shape is what we expect (e,.g. this turns [[18]] into [18])\n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.2797765635793423\n"
     ]
    }
   ],
   "source": [
    "t_Y, t_AL = compute_cost_test_case()\n",
    "t_cost = compute_cost(t_AL, t_Y)\n",
    "\n",
    "print(\"Cost: \" + str(t_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward Propagation Module\n",
    "\n",
    "just as we did for the forward propagation module, we'll implement helper functions for backpropagation. Remember that backpropagation is used to calculate the gradient of the loss function with respect to the parameters.\n",
    "\n",
    "Similarly to forward propagation, we're going to build the backward propagation in 3 steps:\n",
    "<ol>\n",
    "<li> LINEAR backward </li>\n",
    "<li> LINEAR -> ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation </li>\n",
    "<li> [LINEAR -> RELU] X (L-1) -> LINEAR -> SIGMOID backward (whole model)</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = (np.dot(dZ, A_prev.T))/m\n",
    "    db = (np.sum(dZ, axis=1, keepdims=True))/m\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev: [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
      " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
      " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
      " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
      " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
      "dW: [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
      " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
      " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
      "db: [[-0.14713786]\n",
      " [-0.11313155]\n",
      " [-0.13209101]]\n"
     ]
    }
   ],
   "source": [
    "t_dZ, t_linear_cache = linear_backward_test_case()\n",
    "t_dA_prev, t_dW, t_db = linear_backward(t_dZ, t_linear_cache)\n",
    "\n",
    "print(\"dA_prev: \" + str(t_dA_prev))\n",
    "print(\"dW: \" + str(t_dW))\n",
    "print(\"db: \" + str(t_db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Activation Backward\n",
    "\n",
    "Next, we'll create a function that merges the two helper functions: linear_backward and the backward step for the activation linear_activation_backward\n",
    "\n",
    "To help us implement linear_activation_backward, two backward functions have been provided:\n",
    "* sigmoid_backward: implements the backward propagation for SIGMOID unit:\n",
    "\n",
    "dZ = sigmoid_backward(dA, activation_cache)\n",
    "* relu_backward: implements the backward propagation for RELU unit:\n",
    "\n",
    "dZ = relu_backward(dA, activation_cache)\n",
    "\n",
    "if g(.) is the activation function, sigmoid_backward and relu_backward compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_dAL, t_linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"sigmoid\")\n",
    "t_dA_prev, t_dW, t_db = linear_activation_backward(t_dAL, t_linear_activation_cache, activation = \"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
